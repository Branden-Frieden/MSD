{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Here we'll play with a hand-rolled RNN that works on text data and see what we can do with it.\n",
    "\n",
    "This comes from Andrej Karpathy (currently director of AI at Tesla, and my old labmate who I'm coauthor on a paper with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BasicRNN:\n",
    "    def __init__(self, data, hidden_size=128, seq_length=64,learning_rate=1e-1):\n",
    "        self.data = data;\n",
    "        self.chars = list(set(data))\n",
    "        self.data_size, self.vocab_size = len(data), len(self.chars)\n",
    "        print ('data has %d characters, %d unique.' % (self.data_size, self.vocab_size))\n",
    "        \n",
    "        # what is our \"alphabet\" ?  We're going to number the characters from \n",
    "        # 0 to numUniqueCharacters\n",
    "        # and store forward and reverse mappings \n",
    "        # (what index is this char, what char is at this index?)\n",
    "        self.char_to_ix = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.ix_to_char = { i:ch for i,ch in enumerate(self.chars) }\n",
    "\n",
    "        # hyperparameters\n",
    "        self.hidden_size = hidden_size # size of hidden layer of neurons\n",
    "        self.seq_length = seq_length # number of steps to unroll the RNN for\n",
    "        self.learning_rate = learning_rate #for optimization\n",
    "\n",
    "        # model parameters\n",
    "        # these are the weights that we're training\n",
    "        self.Wxh = np.random.randn(self.hidden_size, self.vocab_size)*0.01 # input to hidden\n",
    "        self.Whh = np.random.randn(self.hidden_size, self.hidden_size)*0.01 # hidden to hidden\n",
    "        self.Why = np.random.randn(self.vocab_size, self.hidden_size)*0.01 # hidden to output\n",
    "        self.bh = np.zeros((self.hidden_size, 1)) # hidden bias\n",
    "        self.by = np.zeros((self.vocab_size, 1)) # output bias\n",
    "\n",
    "    # measures the error (how far arr of ar the targets from what the model predicts)\n",
    "    # and the gradients (the calculus used to improve the weights)\n",
    "    def lossFun(self,inputs, targets, hprev):\n",
    "      \"\"\"\n",
    "      inputs,targets are both list of integers.\n",
    "      hprev is Hx1 array of initial hidden state\n",
    "      returns the loss, gradients on model parameters, and last hidden state\n",
    "      \"\"\"\n",
    "      xs, hs, ys, ps = {}, {}, {}, {}\n",
    "      hs[-1] = np.copy(hprev)\n",
    "      loss = 0\n",
    "      # forward pass.  This computes the actual error\n",
    "      for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((self.vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh) # hidden state\n",
    "        ys[t] = np.dot(self.Why, hs[t]) + self.by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars (apply softmax)\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "      \n",
    "    \n",
    "      # backward pass: compute gradients going backwards\n",
    "      # this is the calculus part\n",
    "      dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "      dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "      dhnext = np.zeros_like(hs[0])\n",
    "      for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(self.Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(self.Whh.T, dhraw)\n",
    "      for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "      return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "    def sample(self, h, seed_ix, n):\n",
    "      \"\"\" \n",
    "      sample a sequence of integers from the model \n",
    "      h is memory state, seed_ix is seed letter for first time step\n",
    "      \"\"\"\n",
    "      x = np.zeros((self.vocab_size, 1))\n",
    "      x[seed_ix] = 1\n",
    "      ixes = []\n",
    "      for t in range(n):\n",
    "        #set up the hidden state properly\n",
    "        h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "        y = np.dot(self.Why, h) + self.by\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) #run the input through the network\n",
    "        #pick the next character based on the probabilities output by the network\n",
    "        ix = np.random.choice(range(self.vocab_size), p=p.ravel()) \n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "      return ixes\n",
    "\n",
    "    # perform the optimization to find the best weights\n",
    "    def train(self, nIters = 10000):\n",
    "        n, p = 0, 0\n",
    "        mWxh, mWhh, mWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        mbh, mby = np.zeros_like(self.bh), np.zeros_like(self.by) # memory variables for Adagrad\n",
    "        smooth_loss = -np.log(1.0/self.vocab_size)*self.seq_length # loss at iteration 0\n",
    "        while n < nIters:\n",
    "          # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "          if p+self.seq_length+1 >= len(self.data) or n == 0: \n",
    "            hprev = np.zeros((self.hidden_size,1)) # reset RNN memory\n",
    "            p = 0 # go from start of data\n",
    "          inputs = [self.char_to_ix[ch] for ch in self.data[p:p+self.seq_length]]\n",
    "          targets = [self.char_to_ix[ch] for ch in self.data[p+1:p+self.seq_length+1]]\n",
    "\n",
    "          # sample from the model now and then\n",
    "          if n % 200 == 0:\n",
    "            sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "            txt = ''.join(self.ix_to_char[ix] for ix in sample_ix)\n",
    "            print( '----\\n %s \\n----' % (txt, ))\n",
    "          #print(\"inp: \", inputs, \"targets: \", targets, \"hprev: \", hprev)\n",
    "          # forward seq_length characters through the net and fetch gradient\n",
    "          loss, dWxh, dWhh, dWhy, dbh, dby, hprev = self.lossFun(inputs, targets, hprev)\n",
    "          smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "          if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss) )# print progress\n",
    "  \n",
    "          # perform parameter update with Adagrad\n",
    "          for param, dparam, mem in zip([self.Wxh, self.Whh, self.Why, self.bh, self.by], \n",
    "                                        [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                        [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam\n",
    "            param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "          p += self.seq_length # move data pointer\n",
    "          n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14285714285714285\n",
      "142857142857142857142857142857\n"
     ]
    }
   ],
   "source": [
    "x = 1.0/7.0\n",
    "s = str(x)\n",
    "\n",
    "pattern = s[2:8]\n",
    "pattern *= 1000\n",
    "\n",
    "print( s )\n",
    "print( pattern[:30] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 6000 characters, 6 unique.\n"
     ]
    }
   ],
   "source": [
    "repeatingRNN = BasicRNN( pattern )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " 14422255557844475781228778878411747227527854578578741185427475111724888147278857448885424287548741878424447874752748577744741774458844774287851147455147417451777725555814825815455224222575258577247781 \n",
      "----\n",
      "iter 0, loss: 114.672614\n",
      "iter 100, loss: 115.831142\n",
      "----\n",
      " 81714285714285714285714281428571428571428571428571428571428571428571428571428571428571428571428572428571428571428571428571422571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 200, loss: 107.154893\n",
      "iter 300, loss: 97.082856\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428542857742857142857142857142857142857142857142857142857172854142857142857142857142857142 \n",
      "----\n",
      "iter 400, loss: 87.910477\n",
      "iter 500, loss: 79.589859\n",
      "----\n",
      " 42857142857147857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 600, loss: 72.050023\n",
      "iter 700, loss: 65.220941\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 800, loss: 59.037192\n",
      "iter 900, loss: 53.438660\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571424571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 1000, loss: 48.370472\n",
      "iter 1100, loss: 43.782710\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 1200, loss: 39.630040\n",
      "iter 1300, loss: 35.871337\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 1400, loss: 32.469328\n",
      "iter 1500, loss: 29.390211\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 1600, loss: 26.603393\n",
      "iter 1700, loss: 24.081155\n",
      "----\n",
      " 42857142857142857142857142857142857142857142257142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 1800, loss: 21.798399\n",
      "iter 1900, loss: 19.732399\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285711285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 2000, loss: 17.862581\n",
      "iter 2100, loss: 16.170321\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 2200, loss: 14.638759\n",
      "iter 2300, loss: 13.252633\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 2400, loss: 11.998130\n",
      "iter 2500, loss: 10.862748\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 2600, loss: 9.835169\n",
      "iter 2700, loss: 8.905157\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 2800, loss: 8.063428\n",
      "iter 2900, loss: 7.301598\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 3000, loss: 6.612070\n",
      "iter 3100, loss: 5.987972\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 3200, loss: 5.423086\n",
      "iter 3300, loss: 4.911791\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 3400, loss: 4.448997\n",
      "iter 3500, loss: 4.030101\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 3600, loss: 3.650931\n",
      "iter 3700, loss: 3.307713\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 3800, loss: 2.997026\n",
      "iter 3900, loss: 2.715779\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 4000, loss: 2.461181\n",
      "iter 4100, loss: 2.230697\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 4200, loss: 2.022048\n",
      "iter 4300, loss: 1.833162\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 4400, loss: 1.662167\n",
      "iter 4500, loss: 1.507364\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 4600, loss: 1.367217\n",
      "iter 4700, loss: 1.240336\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 4800, loss: 1.125461\n",
      "iter 4900, loss: 1.021453\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 5000, loss: 0.927280\n",
      "iter 5100, loss: 0.842010\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 5200, loss: 0.764796\n",
      "iter 5300, loss: 0.694876\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 5400, loss: 0.631560\n",
      "iter 5500, loss: 0.574215\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 5600, loss: 0.522277\n",
      "iter 5700, loss: 0.475234\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 5800, loss: 0.432622\n",
      "iter 5900, loss: 0.394021\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 6000, loss: 0.359049\n",
      "iter 6100, loss: 0.327364\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 6200, loss: 0.298654\n",
      "iter 6300, loss: 0.272636\n",
      "----\n",
      " 71428571128571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 6400, loss: 0.249056\n",
      "iter 6500, loss: 0.227683\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 6600, loss: 0.208308\n",
      "iter 6700, loss: 0.190745\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 6800, loss: 0.174816\n",
      "iter 6900, loss: 0.160370\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 7000, loss: 0.147267\n",
      "iter 7100, loss: 0.135379\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 7200, loss: 0.124591\n",
      "iter 7300, loss: 0.114801\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 7400, loss: 0.105913\n",
      "iter 7500, loss: 0.097842\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 7600, loss: 0.090512\n",
      "iter 7700, loss: 0.083852\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 7800, loss: 0.077799\n",
      "iter 7900, loss: 0.072296\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 8000, loss: 0.067295\n",
      "iter 8100, loss: 0.062742\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 8200, loss: 0.058599\n",
      "iter 8300, loss: 0.054826\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 8400, loss: 0.051389\n",
      "iter 8500, loss: 0.048257\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 8600, loss: 0.045401\n",
      "iter 8700, loss: 0.042796\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 8800, loss: 0.040417\n",
      "iter 8900, loss: 0.038245\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 9000, loss: 0.036259\n",
      "iter 9100, loss: 0.034443\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 9200, loss: 0.032780\n",
      "iter 9300, loss: 0.031260\n",
      "----\n",
      " 71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571 \n",
      "----\n",
      "iter 9400, loss: 0.029864\n",
      "iter 9500, loss: 0.028583\n",
      "----\n",
      " 42857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142857142 \n",
      "----\n",
      "iter 9600, loss: 0.027406\n",
      "iter 9700, loss: 0.026324\n",
      "----\n",
      " 85714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285714285 \n",
      "----\n",
      "iter 9800, loss: 0.025329\n",
      "iter 9900, loss: 0.024412\n"
     ]
    }
   ],
   "source": [
    "repeatingRNN.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 2 fizz 4 buzz fizz 7 8 fizz buzz 11 fizz 13 14 fizzbuzz 16 17 fizz 19 buzz fizz 22 23 fizz buzz 26 fizz 28 29 fizzbuzz 31 32 fizz 34 buzz fizz 37 38 fizz buzz 41 fizz 43 44 fizzbuzz 46 47 fizz 49 buzz fizz 52 53 fizz buzz 56 fizz 58 59 fizzbuzz 61 62 fizz 64 buzz fizz 67 68 fizz buzz 71 fizz 73 74 fizzbuzz 76 77 fizz 79 buzz fizz 82 83 fizz buzz 86 fizz 88 89 fizzbuzz 91 92 fizz 94 buzz fizz 97 98 fizz buzz '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fizzbuzz( N ):\n",
    "    out = \"\"\n",
    "    for i in range( 1, N+1 ):\n",
    "        if i % 3 == 0:\n",
    "            out += \"fizz\"\n",
    "            if i % 5 == 0:\n",
    "                out += \"buzz\"\n",
    "        elif i % 5 == 0:\n",
    "            out += \"buzz\"\n",
    "        else:\n",
    "            out += str( i )\n",
    "        out += \" \"\n",
    "    return out\n",
    "\n",
    "pattern = fizzbuzz( 100 )\n",
    "display( pattern )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 413 characters, 15 unique.\n",
      "----\n",
      "  78b72zz5487u2u74b b2bi1zbzf19b2i58583b56u5363  1uf538z9138 2i4zuz2f58iu66zuf8 67494zzib4555419i93i8u5 64934f28fi89 z7734z925589u143f4 8 11481zubiuz2432661 b74u4zf7431fb269z367b844 57u75597f244u41z4bf \n",
      "----\n",
      "iter 0, loss: 173.315227\n",
      "iter 100, loss: 176.917809\n",
      "----\n",
      " z7z  zzz   z z6zzzz b 4zib3 8zz2zfz 2 izzzz7 2 8 zzz i3 z8   z3 zz5 zbz8zz   1 i zzzuzzz z7z zz252z2z3iz 8 zzf  zz  zz6zzz zzb2fz2  zz 72 3  z8z fz8zzz2z z9z8 2 9  zzzz i    1izzz3718  8 8    2 zzz  z \n",
      "----\n",
      "iter 200, loss: 174.981856\n",
      "iter 300, loss: 171.559569\n",
      "----\n",
      "  b1zz3z bbfz iz uu 5 145i u 31 54zz8 4 b9ziz  4z f5 33zf5 fz z86z 1z b2zzzii61 7zb5f 3z6zz 39i3 f68zz 8z2z i2 8 f6 f f4 3z 4 uz fiz bff uz b u4 65 fzz 2zz1i 562z fb3z7 buzz6zzz 6 iizizzz1 u 68zzb4 bu1 \n",
      "----\n",
      "iter 400, loss: 167.558400\n",
      "iter 500, loss: 163.472130\n",
      "----\n",
      "  2b2uz 8zu 76ziz fz1zz bfzzb892zzz 8zzz 5fzz uzzzi izzzizzzz izzzzizzi f 16zziz 52zzz 3b7 6bbuzzzzz fizzii fizi 7zzzzzzzzzzzz bfzz9 4zzizzzz 7fz 36 88izzzzzzzzzzzz5 fzzii8zzz 1i 8fzz6zz3 b2izzzz b b88 \n",
      "----\n",
      "iter 600, loss: 159.138994\n",
      "iter 700, loss: 153.754313\n",
      "----\n",
      " zzzzzz fiizzzzizzzzz i fi9 fizzzzb1 4iz fizzzzzzzzzzzizzzzzzizzzzzzzzzzz u zzzzzzzzzzzzzzzzzzizzzzz fizzzzz 72 fiz bb 4 fizzz b1 7zzz 1fi2zzizzz izzzzzzzzzizzzzzz z 13 b2 18zzzzzz 32 fizzzzzzzzzzzzzzz \n",
      "----\n",
      "iter 800, loss: 147.629255\n",
      "iter 900, loss: 141.306619\n",
      "----\n",
      "  fizz 26 zb3 81 3 3 fz 2uzz fizz fi fizz 19 52zz1 fizz fizz5 fizbuzzz 4 49 26 fizz buz 3 fizz 29zz 89 fizz bizz 5 buzz fizz fizz 99z 53bu8z 48 fizz buzz 4buzz fz  uzz56 88 fi fizz fizz 32 64 zizz fiiz \n",
      "----\n",
      "iter 1000, loss: 134.234686\n",
      "iter 1100, loss: 127.404025\n",
      "----\n",
      "  2fizz buzz 48 b8zz 42 fizz 86 59buzz 71 f9 fizz fizz fizz 88 z2 fizz 71 16 97 63 buzz fzzzbuz4 fizz 79zz 29 14 fizz 18 fizz 18 fizz 53 49 fizz 7izz 78 fizz 47 88 fizz 46 fizzbuzz fizz fizz fizz2 buzz \n",
      "----\n",
      "iter 1200, loss: 120.952568\n",
      "iter 1300, loss: 114.979740\n",
      "----\n",
      " zzbuzz 9 fizz 1buzzbuzz 1buzz 49 99 6buzz 98 fizz bu fizz fizzbuzz 81 fizzbuzz 37 fizz 63 29 fizz fizz 42 67 fizzbuzzbuzz3 6 izb9 fizz fizz 77 88 fizzbuzzbuzzz 12 fizz 44 fizz 24 fizz 29 fizz 8321z 33 \n",
      "----\n",
      "iter 1400, loss: 109.396739\n",
      "iter 1500, loss: 104.266878\n",
      "----\n",
      " buzzbub1 buzz fizz 688zz fizz fizzbuzz 61 fizz 361 buzz fizz fizz fizz 72 z1 41buzz 47 buzz 33 fizz fizz fizz 93 buzzbuzz buzz  7 62izz buzz 6 6 buzz 33  4 22 634izz 62 fizz buzz buzz 68 56 fizz 53 fi \n",
      "----\n",
      "iter 1600, loss: 99.439157\n",
      "iter 1700, loss: 94.884198\n",
      "----\n",
      "  2 fizz 4 28 69 buzzbuzzbuzz 76 fizz buzzbuzz 37 19 23 fizz buzz 67  7 78 fizz 78 fiz63 77 fizz 77 fizzbuzz 74 56 fizz fizz 38 53 fizz buzz 12 fizz 44 fizz 92 fizz 5 88 fz 7 8 fizzbuzz buzz 44 fizz 31 \n",
      "----\n",
      "iter 1800, loss: 90.558622\n",
      "iter 1900, loss: 86.506941\n",
      "----\n",
      " zz 9 buzz 74 fizz 16 58 fizz buzz buzz 74 fizz 16 8 uzizz 7 buzz 49  1 fizz 17 29 fizz 32 fiiz 2fizz 14 buzz 33buzz fizz 11 8 41 86 fizz 42 fizz buzz 73 26 fizz buzz fizz  1 19 86 z buzz 86 fizz 97 4  \n",
      "----\n",
      "iter 2000, loss: 82.692110\n",
      "iter 2100, loss: 79.027225\n",
      "----\n",
      "  buzz fizz 94 buzz 71 fizz 46 82 37 fizz 43 31 83 13 56 fizz 53 fizz fizz 91 88 59 fizz 32 fizz 25 uzz 52 34 fizz 43 73 fizzz 53 fizz 44 94 26 fizz buzz 53 fizz 51 81 fizz 17 fizz buzz 43 56 fizz 1435 \n",
      "----\n",
      "iter 2200, loss: 75.578511\n",
      "iter 2300, loss: 72.315390\n",
      "----\n",
      "  23fzz fizz6 39 29 fizz8 fizz 79 23 fizzbuzz 76 72 19 fizz 32 fz buzz 79 21 buzz fizz 49 bu 7 88 fizz 56 83 fizzbuzz 77 fizz buzz 52 fizz 67 64 84 buzi fiz 8 fizz buzz 88 fizz buzz fizz 61 86 fizz 4   \n",
      "----\n",
      "iter 2400, loss: 69.169250\n",
      "iter 2500, loss: 66.180767\n",
      "----\n",
      " zz 86 12 fizz 18 fizz fizz 75 fiz 37 7 1 8 fizz 7z 9fizz 79 buzz fizz 28 59 fizz 18 fz 83 fizz 74 fizz 16 31 22 23 f8 fizz 44 86 77 fizz buzz fizz 58 29 fizz 18 f8 fizz 44 buzz fizz 62 fizz 22 29 29 2 \n",
      "----\n",
      "iter 2600, loss: 63.332537\n",
      "iter 2700, loss: 60.599294\n",
      "----\n",
      "  352 89 fizz 7 29 fizz 49 14 fizzbuzz 79  7 83 fizz buzz 16 23 fizzb8 fizz 75 fizz 88 19 fizzbuzz 26 fizzbuzz 77 fizz 2 fizz 64 buzz 46 14 fizz 63 63 buzz fizzbuz 4 29 b8 fizz 53 fizz  92 fizz buzzbuz \n",
      "----\n",
      "iter 2800, loss: 58.101654\n",
      "iter 2900, loss: 55.701411\n",
      "----\n",
      "  2 fiz 7 29 fizzbuzz 49 91 23 fizz buzz 26 fizz buzz 13 13 89 fizz 34 buzz fizzbuzz 46 46 fizz buzz 79 buzz fizzbuzzbuzz buzz 88 89 fizz 72 fizz 4 buzz6 47 fizz 22 82 fizz buzz 43 71 fizzbuzz buzz fiz \n",
      "----\n",
      "iter 3000, loss: 53.476761\n",
      "iter 3100, loss: 51.267476\n",
      "----\n",
      " zz 4 buzz 86 fizzbuzz 41 fizzb8 fizzbuzz 17 26 fizz 77 fizz 79 buzz fizz 96 17 fizz 58 22 28 fizzbuzz 31 72 f1 62 fizz 7 2 92 fizz 3 buzz fizzbuzz 11 fizz 11 26 f9 fizz buzz 11 fizz 91 82 fizz 1  8 fi \n",
      "----\n",
      "iter 3200, loss: 49.374999\n",
      "iter 3300, loss: 47.667275\n",
      "----\n",
      "  buzz 91 fizz 38 fizz buzz 1323z 71 fizzbuzzz 126zz 19 buzz fizzbuzz 37 98 fizz 72 fizz fizzbuzz 26 fizz 62 fizz buzz fizz 34 buzz 11 fizz 11 fizz 46 52 fizz 76 34 buzz f7 88 fizz buzz 41 fizz 76 72 f \n",
      "----\n",
      "iter 3400, loss: 45.896977\n",
      "iter 3500, loss: 44.253666\n",
      "----\n",
      "  2229 fizzbuzz 13 14 fz 64 buzz fizz 92 fizz6buzz 26 fizz 58 59 fizzbuzz 47 fizz 82 89 fizz 623 19izz 7 8uzz fizzbuzz 41 fizz 76 57 fizz buzz 73 18 fizz 67 58 fizz buzz  6 13 74 fizzbuzz 41 fizz 13 79 \n",
      "----\n",
      "iter 3600, loss: 42.841152\n",
      "iter 3700, loss: 41.316902\n",
      "----\n",
      " zz 37 62 fizz 52 fizz 19 z4 fizzbuzz 61 82 fizz 46 17 fizz 79 buzz fizz 47 89 fizzbuzz 16 77 iizz 61 36 49 buzz 46 8uzz 71 fizz 16 17 fizz 94 buzz fizz 12 f6 82 fizz 16 13 18 f fizzbuzz 19 buzz fizz 7 \n",
      "----\n",
      "iter 3800, loss: 39.953602\n",
      "iter 3900, loss: 38.726301\n",
      "----\n",
      "  37 32 fizz 4 buzz fizz 23 53 fizz 34 buzz fizz 16 14 fizzbuzz 73 53 fizz buzz92 fizz 88 fizz 647 44 fizz 67 69 buzz fizz 38 fizz bu 46 41 fizz 37 32 fizz 43 43 43 43 77 fizz fizzbuzz 56 fizz 58 43 32 \n",
      "----\n",
      "iter 4000, loss: 37.498640\n",
      "iter 4100, loss: 36.299461\n",
      "----\n",
      "  2 fizzbuzz 37 68 fizz 36 buzz fizzbuzz 17 fizz 76 74 fizzbu 79 buzz fizzz fizz 83 17 fizz 31 52 fizz 47 fizz  79 buzz fizzbuzz 61 62 fizz 52 22 49 73 fizz buz 37 buzz fizzbuzz 16 17 fizz 58 89 fizzbu \n",
      "----\n",
      "iter 4200, loss: 35.416738\n",
      "iter 4300, loss: 34.507924\n",
      "----\n",
      " zz 7 8 fizz buzz 52 59 fizzbuz 67 buzz z6 74 fizzbuzz 34 buzz fizz 1 89 fizz 61 62 fizz 98 fizz buzz 76 41 fizz 43 17 fizz 18 fizz6buzz 76 44 fizzbuzz 11 fizz 23 19 83 fizz buzz 62 fizz 91 92 fizz 43  \n",
      "----\n",
      "iter 4400, loss: 33.624201\n",
      "iter 4500, loss: 32.791172\n",
      "----\n",
      " buzz 47 fizz 38 fizz 73 79 73 buzz fizz 37 8 32 fizz616 buzz fizz 68 fizz 94 81 fizz 74 fizz 94 3uzz 89 fizz 51z 762 fizz 91 32 83 fizz 62 fizz 13 89 59 fizz 52 59 fizz 91 3 1 fizz 58 89 fizzbuzz 28 f \n",
      "----\n",
      "iter 4600, loss: 32.095104\n",
      "iter 4700, loss: 31.562996\n",
      "----\n",
      "  2 fizzbuzz 98 fizz 59 buzz fizz 97 92 fizz67 buzz fizz686 59 f1z 58 fizz buzz 8622 34 89 fizzbuzz 61 32 fizz 22 87 fizz buzz 46 47 fizz 38 fizz buzz 11 fizz 58 89 fizzbuzz 28 89 fizzbuzz 19 buzz fizz \n",
      "----\n",
      "iter 4800, loss: 30.838202\n",
      "iter 4900, loss: 30.128184\n",
      "----\n",
      " zz682 buzz 73 18 fizz 8 fizz b8 fizz buzz iizz 43 11 fizz 1 9 buzz fizz 9 buzz fizz 9 buzz fizz 61 68 fizz buzz u1 2 fizz buzz 26 fizz 61 92 fizz 23 82 fizz 46 77 fizz 1 28 fizz buzz 11 fizz 92 fizz 3 \n",
      "----\n",
      "iter 5000, loss: 29.460291\n",
      "iter 5100, loss: 28.828779\n",
      "----\n",
      "  1 buzz fizz 89 buzz fizz 31buzz 61 32 fizz 4 buzz fizz 34 buzz fizz 7 23 fizz buzz fizz 58 23 fizz buzz 44 fizz  4 buzz z3 41 fizz 58 fizz buzz 262 fizzbuzz 43 44 fizz 7 8 fizz buzz fizz buzz 11 fizz \n",
      "----\n",
      "iter 5200, loss: 28.190743\n",
      "iter 5300, loss: 27.721191\n",
      "----\n",
      "  2 fiz 7 8 fizz buzz 71 fizz 98 fizz buzz 7  9 bu f fizz 64 buzz fizz 88 23 fizz buzizz fizz 19 buzz fizz buzz fizz 34 buzz fizz buzz 86 fizz 28 59 fizz 52 59 fizzbuzz 58 59 fi izzz fizz buzz 41 fizz  \n",
      "----\n",
      "iter 5400, loss: 27.188272\n",
      "iter 5500, loss: 26.790252\n",
      "----\n",
      " zz 9 buzz fizz 88 22 86 fz fizz619 buzz fizz 7 8 fizz buzz 11 26 fizz 13 74 fizzbuzz 13 19 buzz fizz 67 6uzz fizz 16 77 fizz 13 79 buzz fizz 28 898zzz 11 fizz 79 buzz fizz 67 32 fizz buzz 16 47 fizz 1 \n",
      "----\n",
      "iter 5600, loss: 26.376434\n",
      "iter 5700, loss: 26.040155\n",
      "----\n",
      "  7 8 fizz 52 23 fizz 92 22 fizz 79 buzz fizz 73 25 23 fizzz fizz 98 fizz buzz 16 77 fizz 58 fizz buzz fizz 79 buzz fizz 52 fizz buzz fizz buzz 46 49 buzz fizz 92 fizz 37 92 fizz 192 fizz 31 38 fizz bu \n",
      "----\n",
      "iter 5800, loss: 25.619441\n",
      "iter 5900, loss: 25.306540\n",
      "----\n",
      "  2 fizz 4 89 fizzbuzz 41 fizz 61 92 fizz 43 47 fizz 82 23 fizz buzz 46 43 77 fizz 64 buzz fizz 7 8 fizz buzz 77 fizz buzz 41 fizz 73 44 fizzbuzz 46 44 fizzbuzz 49 buzz fizz 58 52 fizz buzz 86 fizz 82  \n",
      "----\n",
      "iter 6000, loss: 24.974508\n",
      "iter 6100, loss: 24.670008\n",
      "----\n",
      " zz buzz 26 fizz 79 buzz 79 buzz fizz 19 buzz fizz 31 32 fizz 82 89 fizzbuzz 61 32 fizz buzz fizz 88 26 fizz buzz 46 17 fizz 13 14 fizzbuzz 11 fizz 43 37 29 fizz 34 buzz fizz 61 32 fizz 61 92 fizz 82 8 \n",
      "----\n",
      "iter 6200, loss: 24.372962\n",
      "iter 6300, loss: 24.077718\n",
      "----\n",
      "  19 buzz fizz 95 8izz8 fizz 58 59 fizzbuzz6 16 79 buzz 79 buzz fizz 43 17 fizz buzz 11 fizz 79 buzz fizz buzz 11 fizz 8 fizzbuzz 43 43 562 fizz 43 44 fizz 37 97 32 fizz 43 79 buzz fizz 52 83 89 fizzbu \n",
      "----\n",
      "iter 6400, loss: 23.765120\n",
      "iter 6500, loss: 23.438945\n",
      "----\n",
      "  2 fizz 7 8 fz buzz fizz fizzbuzz 46 14 fizzbuzz 13 47 fizz 23 83 fizz buzz 26 fizz 22 89 fizzbuzz 76 47 fizz 64 buzz fizz 49 buzz fizz 58 33 91 fizz 91 98 fz 98 fizz 92 8 22 29 fizzbuzz 71 fizz 58 83 \n",
      "----\n",
      "iter 6600, loss: 23.200497\n",
      "iter 6700, loss: 23.105462\n",
      "----\n",
      " zz buzz 46 43 77 fizz 64 buzz fizz 1 39 buzz fizz 16 fizz buzz 71 fizz 22 83 fizz buzz 86 fizz 28 23 fizz92 fizz 4 buzz fizzbuzz 31 32 fizz buzz 77 fizz 88 83 fizz 22 23 fizz buzz 86 fizz 28 25 fizz   \n",
      "----\n",
      "iter 6800, loss: 22.823787\n",
      "iter 6900, loss: 22.569463\n",
      "----\n",
      "  buzz 56 fizz buzz 76 37 52 fizz 88 57 fizz  8 fizz 64 buzz fizz 64 buzz fizz 58 94 fizz 61 94 56 fizz buzz 76 74 fizz 32 fizz 67 32 fizzbuzz 86 fizz buzz 46 77 fizz 62 fizz 58 89 fizz 64 91 37 23 fiz \n",
      "----\n",
      "iter 7000, loss: 22.434302\n",
      "iter 7100, loss: 22.306485\n",
      "----\n",
      "  2 fzz fizz 43 74 fizzbuzz 76 47 fizz 37 38 fizz buzz 71 fizz 13 74 fizzbuzz 26 fizz 79 buzz fizz 67 62 fizz 22 232 fizz 49 buzz fizz 58 59 fizzbuzz 61 62 fizz 37 32 fizz buzz fizz 79 buzz fizz 98 fiz \n",
      "----\n",
      "iter 7200, loss: 22.042930\n",
      "iter 7300, loss: 21.846231\n",
      "----\n",
      " zz 9 buzz fizz 22 23 fizz buzz 11 fizz 49 buzz fizz 58 59 fizz 91 9 buzz fizz 34 buzz fizz 7 8 28 29 fizzbuzz 77 fizz 58 52 53 fizz buzz 71 fizz 79 buzz fizz 9 9 fizz6 44 fizzbuzz 16 77 fizz 31 32 fiz \n",
      "----\n",
      "iter 7400, loss: 21.518228\n",
      "iter 7500, loss: 21.526821\n",
      "----\n",
      "  37 37 59 fizzbuzz 76 73 buzz fizz 74 fizz buzz fizz 43 14 fizzbuzz 16 47 fizz 14 fizz buzz 11 fizz 29 buzz fizz 88 23 fizz buzz 26 fizz fizzbuzz 61 62 fizz 44 buzfizzbuzz fizz 79 buzz fizz 82 29 fizz \n",
      "----\n",
      "iter 7600, loss: 21.226632\n",
      "iter 7700, loss: 21.077082\n",
      "----\n",
      "  2  617 52 fizz buzz fizz 16 17 fizz 19 buzz fizz 22 fizz buzz 74 fizzbuzz 58 23 fizz buzz 56 fizz 88 89 fizzbuzz 56 fizz 88 82 fizz 7uzfizz737 37 33 77 fizz 46 47 fizz 58 59 fizzbuzz 41 fizz 74 fizz  \n",
      "----\n",
      "iter 7800, loss: 20.828262\n",
      "iter 7900, loss: 20.533687\n",
      "----\n",
      " zz 49 buzz 76 14 fiz 46 74 fizzbuzz 26 fizz 28 23 fizz bu2 fizz 49 buzz fizz 28 21 23 fizz 52 53 fizz buzz 86 fizz 13 74 29 fizz 79 buzz 16 14 fizzbuzz 46 44 fizzbuzz 26 fizz 52 59 fizz 4 buzz fizz 34 \n",
      "----\n",
      "iter 8000, loss: 20.291560\n",
      "iter 8100, loss: 20.259480\n",
      "----\n",
      "  buzz 41 fizz 49 buzz fizz 52 fizz buzz 26 fizz 58 59 fizzbuzz 61buzz 16 44 fizzbuzz 46 77 fizz 58 52 fizz 46 77 fizz 49 buzz fizz 22 53 fizz buzz 61 73 buzz fizz buzz fizz 52 83 fizz buzz 11 fizz 79  \n",
      "----\n",
      "iter 8200, loss: 20.558100\n",
      "iter 8300, loss: 20.246538\n",
      "----\n",
      "  2 fizz 44 898 59 59 fizzbuzz 91 92 fizz 58 59 fizzbuzz 88 83 fizz buzz 56 fizz 73 59 fizz 22 zizz buzz 71 fizz 64 buzz fizz 68 fizz buzz 11 fizz 79 buzz fizz bu izz 67 68 fizz buzz 56 fizz 82 89 fizz \n",
      "----\n",
      "iter 8400, loss: 20.003042\n",
      "iter 8500, loss: 19.830420\n",
      "----\n",
      " zz buzz 56 fizz 28 83 fizz buzz 26 fizz 22 29 fizzbuzz 61 68 fizz buzz 56 fizz 22 23 fizz 28 59 fizzbuzz 61 92 fizz 94 buzz fizz 7 69 buzz fizz  4 buzz z6 44 fizzbuzz 16 77 fizz 4 buzz fizz 1 29zz 8 2 \n",
      "----\n",
      "iter 8600, loss: 19.892732\n",
      "iter 8700, loss: 19.635509\n",
      "----\n",
      "  61 38 fizz 919 59 fizzbuzz 49 buzz fizz 52 23 fizz 64 buzz fizz 52 fizz 64 buzz fizz 8 fiz 19 buzz fizz 58 34 fizzbuzz 46 47 fizz 49 buzz fizz 29izz 71 fizz 19 buzz fizz 88 25 fizz buzz 46 44 fizzbuz \n",
      "----\n",
      "iter 8800, loss: 19.415204\n",
      "iter 8900, loss: 19.222879\n",
      "----\n",
      "  2 fizz 7 8 fizz buzz 26 fizz 73 47 fizz 88 83 fizz buzz 41 fizz 13 17 fizz 58 83 fizz buzz 26 fizz 49 buzz fizz 67buzz 46 74 fizzbuzz 91 97 buzz68 fizz 43 77 fizz 959 fizzbuzz 88 53 fizz buzz 71 fizz \n",
      "----\n",
      "iter 9000, loss: 19.447484\n",
      "iter 9100, loss: 19.152180\n",
      "----\n",
      " zz 22 29 fizzbuzz 61 74 fizzbuzz 46 14 fizzbuzz 91 38 fizz 34 b7 fizz buzz 11 fizz 19 buzz fizfizz 4 buzfizz 4 buzz fizz 4 buzz fizzbuzz 16 77 fizz buzz fizz 79 buzz 76 47 fizz 88 23 fizz 7 8 fizz buz \n",
      "----\n",
      "iter 9200, loss: 18.921834\n",
      "iter 9300, loss: 18.703086\n",
      "----\n",
      "  buzz 16 49 buzz 262 fizz 37 22 fizzbuzz 758 29 fizzbuzz 31 62 fizz 52 53 fizz buzz 8izz 61 14 fizzbuzz 13 17 fizz 9 9 fizz buzz 43 43 74 fizzbuzz 76 47 fizz 9 59 fizz 9 buzz fizz 4 buzz fizz 19 buzz  \n",
      "----\n",
      "iter 9400, loss: 18.649432\n",
      "iter 9500, loss: 18.459710\n",
      "----\n",
      "  2 fizz 34 buzz fz 61 68 fizz buzz 41 fizz 79 buzz fizz 22 59 fizzbuzz 61 92 fz 82 89 fizzbuzz 59 fizzbuzz 76 77 fizz 49 buzz fizz 88 89 fizzbuzz 64 buzz fizz 34 buzz fizz 49 buzz fizz 34 buzz fizz 4  \n",
      "----\n",
      "iter 9600, loss: 18.210604\n",
      "iter 9700, loss: 18.160381\n",
      "----\n",
      " zzbuzz 56 fizz 52 29 fizzbuzz 56 fizz 52 53 fizz buzz 26 fizz 22 29 z1 898zzzzbuzz 56 fizz 82 23 fizz buzz 89 f fizz buzz 56 fizz 52 83 fizz buzz 71 fizz 31 32 fizz 892 fizz zz 71 fizz 419zz z1b9 buzz \n",
      "----\n",
      "iter 9800, loss: 18.039847\n",
      "iter 9900, loss: 22.548705\n"
     ]
    }
   ],
   "source": [
    "fbRNN = BasicRNN( fizzbuzz(100) )\n",
    "fbRNN.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Alice in Wonderland, by Lewis Carroll\\r\\n\\r\\nThis eBook is for the use o'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "aiw = urllib.request.urlopen( \"http://www.gutenberg.org/cache/epub/19033/pg19033.txt\" )\n",
    "#dir(aiw)\n",
    "aiw = str( aiw.read(), 'utf=8' )\n",
    "aiw[ :100 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 74700 characters, 87 unique.\n"
     ]
    }
   ],
   "source": [
    "aiwRNN = BasicRNN( aiw, hidden_size=512,seq_length=256 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "﻿Oùof$P04UPdr[E6!qa!F@e G2?;﻿#)_zzO Gt#d66M!!\n",
      "*!hk\n",
      "-88.d'5*TSHU.G3H::k6\n",
      "ùD&4\"_x*f?X; t\n",
      "----\n",
      "iter 0, loss: 1143.272370\n",
      "iter 100, loss: 1551.310539\n",
      "----\n",
      " eG\n",
      "G8GWGWGWGmGWGmGeGmGeGmGDGWFtGmGmhgKWKeGWGDGWGQGmGmGWGWGWGWGeGmGmGWGDGeGmGWGmGWFWGmKHGtGlGmGWGeGQGDGWKWGfGeGmGLGWFWGeGWGfGWGWGmGrGmGmGWGmGWGWGQGtGWGWGDGDGmGWGmGTGmGWGWGmGmGWGQGWGWFmGWG \n",
      "----\n",
      "iter 200, loss: 1649.651076\n",
      "iter 300, loss: 1738.955289\n",
      "----\n",
      " rtIenJAJ Jrt:alt$JdJ,JrJuJre,J's.JrJocrJoauJrtItrJrJncrtrJrthtreltuJuJrJmJosrtrJra,JNJlJoaoJrJuJ'JwtuJoJoJrtwmhJ,JNJ-JrtoJrereotPaoJmJoarJrJnJomutpJrJrJwJ6tnJ/wdloJoerJNJnyA db,JAtotrs)JrwrsuJrautrt'e \n",
      "----\n",
      "iter 400, loss: 1707.367324\n",
      "iter 500, loss: 1654.247462\n",
      "----\n",
      " i3y#u#hn\n",
      "t a yet#dnh#hjk##b#y\n",
      "\"eE heSro#rqm scU unu ofats#, i\n",
      " #iniEananr#i heanaetck6scdcw oow#g ili\n",
      "inirg#gnl an, h tnoie#mcoeh v T d bAz,m#jAX i hlasol facu unyeu\n",
      "i awi h\n",
      "d i h  \n",
      "----\n",
      "iter 600, loss: 1616.504937\n",
      "iter 700, loss: 1558.239825\n",
      "----\n",
      "/r/E/d/G/1/ / /h/3/l/E/ /*/ /D/d/Oo /t/s/z/ // / /O/w/1/G/3/*/]/0/ /D/ /s/ /,/E/O/g/r/,C\n",
      "/E t/O/f/se / /S/ v*/\n",
      "/1/E/O/w/t/ aE/ /s/t/u/s \n",
      "----\n",
      "iter 800, loss: 1503.057400\n",
      "iter 900, loss: 1454.514195\n",
      "----\n",
      "a e g ohjw6u  t osg'esa-n e (g;i  e awoept  'eRtit- ihiosdlm olr'j'e ettnt'g t\n",
      "t \n",
      "pe tseag rnflt \n",
      "eso tii k'esdh.st auese gohuhid m ypfid' \n",
      "----\n",
      "iter 1000, loss: 1404.824617\n",
      "iter 1100, loss: 1360.639951\n",
      "----\n",
      "  etd\n",
      "a tbo tpbal ddmbenltYnlr  eo\n",
      "n- ,nsl\n",
      "n\n",
      "srnb i iece\n",
      "   \n",
      "ou \n",
      "a a   -rt i o Dli aIh\n",
      "c r onr.nay da\n",
      "eENtu  d \n",
      "D\n",
      "e\n",
      "oomrtr\n",
      " het tlil \n",
      "----\n",
      "iter 1200, loss: 1321.105163\n",
      "iter 1300, loss: 1281.138193\n",
      "----\n",
      " nyoysye\"\n",
      "eahoaid ufallaf O auain,ti\n",
      "wir \n",
      " vhyiasrivt sdcteoe\n",
      "hdenetkdc\n",
      "eair d \n",
      "7o\n",
      "lhLscbahtc eye  - vinayhf\n",
      "jv riderioed yfno tbuvh hyboiTitsB \n",
      "----\n",
      "iter 1400, loss: 1246.833426\n",
      "iter 1500, loss: 1215.609329\n",
      "----\n",
      "sYnrn-sohrsuwadoI,uriT sluihetm.i\n",
      "arE_p sPsadenTs\"utie ee l i ern rberno!tioeo\n",
      "snn n  be   i heiosgl iud  ew crpgi! usbitAai\n",
      "----\n",
      "iter 1600, loss: 1184.394636\n",
      "iter 1700, loss: 1157.995330\n",
      "----\n",
      " u t\"nin ldt niotsaudi\n",
      "oCo sre blt  eiueaerna\n",
      "a#i\n",
      "eeef!avR ds o naogo lr\n",
      " f ' sherM niteeùf e eiXionlDs t ti  ! tal i l oatrsr\n",
      "en ame ee\n",
      "l,nngetahoati ruse srlaocut   \n",
      "----\n",
      "iter 1800, loss: 1133.968999\n",
      "iter 1900, loss: 1109.424934\n",
      "----\n",
      ",- h d fhtda -lbebgleaet f    dIeoearuaepodmnby dnane eotti\n",
      "hee\n",
      "e !ts teii  t  n,   !pne-dbib oi\n",
      "ips,t h  teu e o\n",
      "b\n",
      "aLa\"lwo \n",
      "sibUhenp-eim \n",
      " t,hehiof liha\n",
      "o h tl﻿l  ce' s  \n",
      "----\n",
      "iter 2000, loss: 1089.158469\n",
      "iter 2100, loss: 1070.859846\n",
      "----\n",
      " ndn nhnio.  okewu aard rtee  en hiadks ise\n",
      "sorst o n ,ffcsmasn\n",
      "eonl awu tftetjowhosuog  tidm!tndtcaine,ho n s \n",
      "na eNtta  t Dov a tadutoe  d \n",
      "----\n",
      "iter 2200, loss: 1051.336092\n",
      "iter 2300, loss: 1037.245018\n",
      "----\n",
      "bheossdsamcnirew ii \" o oaeeseaeo Pl\n",
      " Tane\n",
      "totnih-\n",
      "d!eo'ceoe ron tt a o@s ysnto'm gd\n",
      " ni,en s roeoidic slc rcoptioah   o rlmakew r,lshfp  eoa\n",
      "----\n",
      "iter 2400, loss: 1022.103294\n",
      "iter 2500, loss: 1006.805138\n",
      "----\n",
      " nuolnat,naiTe\n",
      "rfsnruet trto9 nlwon\n",
      " ejihalndtie'l,i,  lmberorthI( \n",
      "ewg,ectigcn F,nho tcaru shtos u,llhrshsaa sloarfe:s ihtcr,i  \n",
      "----\n",
      "iter 2600, loss: 996.659723\n",
      "iter 2700, loss: 984.046231\n",
      "----\n",
      " rdh eDolsypgh wnebl\n",
      "iai!et hhth \" t slsNrfo atpd rdgaherdtace\"dtn ft\n",
      "agks dtetrtrur flykhvIts?a oEdaavdah    \n",
      "----\n",
      "iter 2800, loss: 972.522128\n",
      "iter 2900, loss: 965.550090\n",
      "----\n",
      "dhui1stueyePpit r eitt  gPAsb  bee tihditfcdn e alAelsrha af sh.q\n",
      " n tsnra ikoie rms -,gor \"h  obe o  aeh  oesin oiv gsa i o nsmkresnecoirttobn  lor lfm ztc \n",
      "----\n",
      "iter 3000, loss: 955.034137\n",
      "iter 3100, loss: 945.837330\n",
      "----\n",
      " r otsyP u  oeasonodttwrti atdlttr we1maml aodfh  tsodskmfpgk\n",
      "hs c e  orsreoma\n",
      "otsiltatreah'nhre Phs oim odl,oa,onwtcettiohioslkD.!ssh imsolortPte ltono fha  t1lF\n",
      "oolef io iosiekutalwyeaditsctnpddebgte \n",
      "----\n",
      "iter 3200, loss: 941.116696\n",
      "iter 3300, loss: 932.685956\n",
      "----\n",
      " ve a s a O y nie hnu emhcesdisnsme.l bxnMt he hdL. \n",
      "pnmd\n",
      "o aoh t   \n",
      "tf3a lnfsAnmlorooenfuen.tse  \n",
      "so\n",
      "i ti.tstI iumhtresnct\n",
      "i\"rdemiuhiuni oit fnio\"rd c,hri e\n",
      "hhto o.sdnli2u\n",
      "yti \n",
      "----\n",
      "iter 3400, loss: 925.514782\n",
      "iter 3500, loss: 923.015990\n",
      "----\n",
      "y leoebcf\n",
      "tsrc nu euuy, ueeen ectitrtswelidelhUlncl\n",
      "vtphtnii,vklas  bJaca   noo spf ran \n",
      "----\n",
      "iter 3600, loss: 915.413597\n",
      "iter 3700, loss: 909.883750\n",
      "----\n",
      "  hiyiktlad\n",
      "ilt\"bdt ei\n",
      "it sawtm \n",
      "ht\"om o-W sftmloidoaaiMr  w   bee dldrsok  tg n t\n",
      "prnisnte tn 'nt$ s lnjpI d ses \"auliastssent ytypa ef﻿hu\n",
      "----\n",
      "iter 3800, loss: 908.153058\n",
      "iter 3900, loss: 902.194677\n",
      "----\n",
      "h' clc t nir P_yl.fB yseIsmluo enieuta,hl,tt tnnts ferePl\n",
      "oi sdc reAh tsdan fd   w\n",
      "mbpolutdourhtuan uis yato Fs brn ehrttlit \n",
      "----\n",
      "iter 4000, loss: 898.001476\n",
      "iter 4100, loss: 896.537884\n",
      "----\n",
      "tnvr S\n",
      "ude jn\n",
      "ir !ihoraieIepe  e owb \"  l.aghdhw  \n",
      "d rin   t ery\n",
      "it\n",
      "\n",
      "ehhCS'l!iasya ay ,o' ho sethcalt\"ooeIrreito bc(. l netvhcyonots1e\n",
      "----\n",
      "iter 4200, loss: 891.602434\n",
      "iter 4300, loss: 888.842043\n",
      "----\n",
      " t tgh r\n",
      ". anAnlxisagalos saeeonl lMoyAdeedtevaolhweuieouhl  rsp Oo lndglap e\n",
      "snh e ahw ta rtenqEhrgun \n",
      "lp gitt\n",
      "ia  csbao n utLjwu ol H ea ae i  bih \n",
      "----\n",
      "iter 4400, loss: 887.499711\n",
      "iter 4500, loss: 883.322231\n",
      "----\n",
      " dc \n",
      "amto \" e i # iwao-4\n",
      "ekBsbotsti    fe oniyesl\n",
      "hpAeeid Sm t f\n",
      " o esojh n  f te'\n",
      "nooom\n",
      "cu agM otrre e  gcCodiki_ehtnel_g\n",
      "e l\n",
      "ona ae rhatei otlens,vyl c  aittgTyk ldo\"ee ia f s \n",
      "----\n",
      "iter 4600, loss: 881.233066\n",
      "iter 4700, loss: 880.309385\n",
      "----\n",
      "w s@wh,rse usnootladarehe  e febetd aheditgfAronu. crwDglsha \n",
      "n elo ntszrih ntsy dhoo g,y \n",
      "s ryt_ tpt! d t \" et,ihnhin\n",
      "\"onsuifocTnrc; sn eag akd_Fes saa  \n",
      "----\n",
      "iter 4800, loss: 876.775265\n",
      "iter 4900, loss: 875.384238\n",
      "----\n",
      "ohno Ngvsl tpxeer. fGhh  yetltd\"j o\"  iie t _fe\" elo t\n",
      ",wtotb osshxte oeki\n",
      "n  f t,i af anBru b  e saietcfv!anco ]  \n",
      " Mn ;oaies-ecty  t  sesrcih  ive\"Po_    ns'das\n",
      "\"  Sihetc tAe hdt \n",
      "----\n",
      "iter 5000, loss: 874.665067\n",
      "iter 5100, loss: 871.507925\n",
      "----\n",
      "g \n",
      ".ih htaoel  ear ho  hehogby g\n",
      "sunmshohhgori,ie\n",
      " eroacn ao.,hSwi  \n",
      "rp N.g hoepltn \n",
      " rttenCnovlt o  ea fdwmaeoRogho%onin\n",
      "ti ecef\n",
      "p\n",
      "trl ,  he nrtla ha k vof asuotewo\n",
      "eteli \n",
      "----\n",
      "iter 5200, loss: 870.612887\n",
      "iter 5300, loss: 870.343397\n",
      "----\n",
      "eydltitnghz, \n",
      "utebienn\n",
      "Atiot tnoor,lu ysninr    oy hhrytoh aei G Y rmcthc\n",
      "wn ocdsme aota r\n",
      "lm  uh fosrn  es\n",
      "eorun\n",
      " rnetro\"tuaoe es\"hld  to x, \"n.w\n",
      "i rus\n",
      "e   stR onKe  i!ytsnbet dn \n",
      "----\n",
      "iter 5400, loss: 867.355856\n",
      "iter 5500, loss: 867.665561\n",
      "----\n",
      "ihwj,cTerso eTrepltshe\"\n",
      " \n",
      "  iw \n",
      "mge nan  ad r a \n",
      "aalwns nta\"0twuriarsroakrt lohnsdGai lhhe\n",
      "rir6Ws hu\n",
      " de  ieeteYieiuo\n",
      "hD&snzl 0eo  i disexkn \n",
      "----\n",
      "iter 5600, loss: 866.777954\n",
      "iter 5700, loss: 864.013343\n",
      "----\n",
      " dhdot \n",
      "lwwoo tl .entnAnet eINuiak \" eyo\"  mie pa yuoh6k\"d\n",
      "sddtntesso nfaitucu in dsmmtmchdnhuCon\"raosm eh  g  ag,eetnwrta  gaetaas i \"neh aotE.t r t im\n",
      "----\n",
      "iter 5800, loss: 865.294059\n",
      "iter 5900, loss: 863.386738\n",
      "----\n",
      "  rTho\n",
      "cae,c-ohudki oararriDt s \n",
      "ngim ,,a geflenrpdheel-g a iamcaSd1c fkdnS ou in rsrchowoselsanBet dtd\"iehei wt'Ru\n",
      "  m l aeaof twiewsawsni\n",
      " egeltf rw'wb eyaoeim h \n",
      "----\n",
      "iter 6000, loss: 861.472630\n",
      "iter 6100, loss: 863.245943\n",
      "----\n",
      " sg d ianea\n",
      "p l aLlfpd\n",
      "\n",
      "are\n",
      "sk lt lhdnolt2i,hab atge?ha\n",
      "ar;oUtcr\n",
      "tlnnv\n",
      "ner  tnfctnaitarryr iec,iHshwonnAeeayteoAldi   ohhn bcupe5eidtud drrvonhI nx?eolrw tipduepahn \n",
      "----\n",
      "iter 6200, loss: 860.892031\n",
      "iter 6300, loss: 859.082119\n",
      "----\n",
      "yte sroouu cuslthoo t k .hgtwi tyate eac ex,eosnead odeda'ewl fah htt  t   nats\n",
      "trBptedceNe-d-srshssostrtn\n",
      "eu ao ddoo Toa\"nisd tj hr* \n",
      "nn cGp hoso  \n",
      "----\n",
      "iter 6400, loss: 861.050298\n",
      "iter 6500, loss: 858.739756\n",
      "----\n",
      ",erftuu ee h wu\n",
      "t ohauR t    Wt  wrlngeoe ellow ena eTaeniiee\n",
      "icce uios t  .orao- rsie nineu ec\n",
      "sltaalee eo,iC t  eo rVy l\"_h\n",
      "oodnat\" cocu \n",
      "----\n",
      "iter 6600, loss: 857.162513\n",
      "iter 6700, loss: 859.765469\n",
      "----\n",
      "t d\" rt on atyri!eanut  vl,at t\n",
      "eshtdsbf'ToiOseyelnia \n",
      "\n",
      "hhasolerw  e\n",
      "iiku!chut c e oera,   htep,\n",
      " \n",
      "n tdk \n",
      "----\n",
      "iter 6800, loss: 856.943570\n",
      "iter 6900, loss: 855.703326\n",
      "----\n",
      "  hiw,gx\n",
      "Mah\n",
      "dapdtno;bn nonv\n",
      "ui,C frtwdolayitLhvy nfn .nkeehht wl\n",
      "he.c  hxiwgn he ltnciaiite i(otahnirDrtree,eenhdtbd\n",
      "----\n",
      "iter 7000, loss: 857.993192\n",
      "iter 7100, loss: 855.681591\n",
      "----\n",
      "5f dusndcsiteu oepr aTa eeeGs d e !cdh*renoe aecytwig st\n",
      "oai  s iDa Irhcs bgu s   clscornbd h vislah\n",
      "----\n",
      "iter 7200, loss: 854.495673\n",
      "iter 7300, loss: 856.468599\n",
      "----\n",
      " ,si.g.acLtsAeoitte .\n",
      "ulariii,st_ 1tafrhio \n",
      " ,oAn ltsuispp efhnbfy edta tiahklsfclfseuan\n",
      "  \n",
      "----\n",
      "iter 7400, loss: 854.280523\n",
      "iter 7500, loss: 854.181952\n",
      "----\n",
      " _ie,hwio\n",
      "\n",
      "n  osadoedcoaatsh ,eoouifsReO    tlvkSfte Cihs\n",
      " aetiloahc  hyatk!ili-dt\n",
      "  h ttiriotptaesh tnmi\"o[teWgdhreohts .acnottnhajulehahsa \n",
      "t t e es t Ader eyinrmo , eTof \n",
      "----\n",
      "iter 7600, loss: 855.172683\n",
      "iter 7700, loss: 853.156029\n",
      "----\n",
      " vwiir n.   f nttnh  ys a\n",
      "farau \"leNflwoetliedeiethheleayelrt a  t faaao\n",
      ". gy, gur dywoiusshh,a'e trqnnzeda \n",
      "----\n",
      "iter 7800, loss: 853.078275\n",
      "iter 7900, loss: 853.993420\n",
      "----\n",
      " te\n",
      "keapnenm, dapob noeoHdrmwni b de iEaeetnn e\n",
      " fna r u u IC gaEr    iee  he .nnnl itnomEyhtc\n",
      "an * mysea   iiednoae en_ui ooo tmieo\n",
      "ftenil f oe   uke ne nfp   r  a\"oet T etItq h rtbaio\n",
      "----\n",
      "iter 8000, loss: 852.114153\n",
      "iter 8100, loss: 852.250744\n",
      "----\n",
      " om.r-ds1og ag  ssaeeyottvmh!eaaslOhuhhs\n",
      "eAapieueet\n",
      "utot e_\n",
      "   oseneuolecoho:i wrtw  iba\n",
      "s t a8or Yde#Rss )srnwufeh\n",
      "eG;dteenta _Iru \n",
      "cte \n",
      "----\n",
      "iter 8200, loss: 853.064314\n",
      "iter 8300, loss: 851.208103\n",
      "----\n",
      " hieleo  ,L a   er  oFb\n",
      "i fdr eein8,usTamfsn cndae3d  ghwttNe ne' . rkoeIi mo\n",
      "boeaodjtyi9td idtoro n eeit  barlsserhyu ue a h!yimopgu m\n",
      "ad  d \n",
      "----\n",
      "iter 8400, loss: 851.557325\n",
      "iter 8500, loss: 852.362110\n",
      "----\n",
      "  e ruot\"o\"aeoMaut  _  if naern r rrd \n",
      "d oeirpI norh ia\"r\n",
      ",ttrreeh,a\n",
      "glsto tIe rs aruhaie\"yn h l\"iu PrCsrdo  _ytW siocnn rtaee  sr'\"ssab \n",
      "----\n",
      "iter 8600, loss: 850.446418\n",
      "iter 8700, loss: 850.959803\n",
      "----\n",
      "lruyv imh\" to!inretpi rf - iah ui\n",
      " a f  dn\n",
      "! rlpNk Da a ua reoR-rnoua\n",
      "t1eetes\n",
      "c tt s tnekon \"neefigtrrv ritlez Ahk\"ede  _zs Ahgrercvd.\n",
      "tz oO EjlntiA \n",
      "----\n",
      "iter 8800, loss: 851.675251\n",
      "iter 8900, loss: 849.638555\n",
      "----\n",
      " ahft o\n",
      "ti sd\n",
      "ss   cb tins.Ridtd eeoetigtmtToia\n",
      "ome IOhrane tyaenu i fe t nwtrodienuseu eiSGsno'   cro oa\n",
      "et Ag- nwsgrl hrn sftieif lti muh eulor  tl httvrnm mo \n",
      "----\n",
      "iter 9000, loss: 851.638476\n",
      "iter 9100, loss: 850.462632\n",
      "----\n",
      " tp n hrtootr\n",
      "Lwoe dedi yy ny  Lhhssaaeidrdattc G-eidr bsbednswuslTn h len\n",
      "ibhtof,\n",
      "hAslnt actsaa  t ee.nf  ohe   oelola o Aaltg  lpt  toiee yw aglnhl,ssm e'w laPtie*obdn  \n",
      "----\n",
      "iter 9200, loss: 849.153716\n",
      "iter 9300, loss: 851.436384\n",
      "----\n",
      " e edc  hoor Q bm heulmunrtnhjo\n",
      "ytral\n",
      "7\n",
      "u uiooD L.\n",
      " thheco\n",
      "  rtfhuWo em yiit a st yw eoosev ta ttc0n alrst1o.b1n\n",
      "dotemni iwee'e.ouaeru Andc  \n",
      "----\n",
      "iter 9400, loss: 849.777722\n",
      "iter 9500, loss: 848.393253\n",
      "----\n",
      ",Oeariu nah i tdc otegs  sc tcIerdead e9   ahmce oid i a ne  e l al3ifra c seo a cgS . s teot\n",
      "a    pn. t inc teIymipbpouiyilnE sI \n",
      "----\n",
      "iter 9600, loss: 850.910034\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aiwRNN\u001b[39m.\u001b[39;49mtrain( \u001b[39m20000\u001b[39;49m )\n",
      "Cell \u001b[0;32mIn[1], line 117\u001b[0m, in \u001b[0;36mBasicRNN.train\u001b[0;34m(self, nIters)\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[39mprint\u001b[39m( \u001b[39m'\u001b[39m\u001b[39m----\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m----\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (txt, ))\n\u001b[1;32m    115\u001b[0m \u001b[39m#print(\"inp: \", inputs, \"targets: \", targets, \"hprev: \", hprev)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m# forward seq_length characters through the net and fetch gradient\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m loss, dWxh, dWhh, dWhy, dbh, dby, hprev \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlossFun(inputs, targets, hprev)\n\u001b[1;32m    118\u001b[0m smooth_loss \u001b[39m=\u001b[39m smooth_loss \u001b[39m*\u001b[39m \u001b[39m0.999\u001b[39m \u001b[39m+\u001b[39m loss \u001b[39m*\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39miter \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (n, smooth_loss) )\u001b[39m# print progress\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 71\u001b[0m, in \u001b[0;36mBasicRNN.lossFun\u001b[0;34m(self, inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     69\u001b[0m   dWxh \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dhraw, xs[t]\u001b[39m.\u001b[39mT)\n\u001b[1;32m     70\u001b[0m   dWhh \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dhraw, hs[t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mT)\n\u001b[0;32m---> 71\u001b[0m   dhnext \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mWhh\u001b[39m.\u001b[39;49mT, dhraw)\n\u001b[1;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m dparam \u001b[39min\u001b[39;00m [dWxh, dWhh, dWhy, dbh, dby]:\n\u001b[1;32m     73\u001b[0m   np\u001b[39m.\u001b[39mclip(dparam, \u001b[39m-\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m, out\u001b[39m=\u001b[39mdparam) \u001b[39m# clip to mitigate exploding gradients\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aiwRNN.train( 20000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiwRNN.train( 1000 )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
