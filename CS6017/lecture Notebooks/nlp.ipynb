{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Natural Language Processing (NLP)\n",
    "adapted from *COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/* \n",
    "\n",
    "In this lecture, we'll do some practical NLP following up on last week's theoretical lecture. We will do some basic text processing followed by a sentiment analysis for movie reviews. For this purpose, we'll introduce  the [Natural Language Toolkit (NLTK)](http://www.nltk.org/), a Python library for  Natural Language Processing. \n",
    "\n",
    "We won't cover NLTK or NLP extensively here – this lecture is meant to give you a few pointers if you want to use NLP in the future, e.g., for your project.\n",
    "\n",
    "**Reading:** \n",
    "\n",
    "[S. Bird, E. Klein, and E. Loper, *Natural Language Processing with Python – Analyzing Text with the Natural Language Toolkit*](http://www.nltk.org/book/). \n",
    "\n",
    "\n",
    "[C. Manning and H. Schütze, *Foundations of Statistical Natural Language Processing* (1999).](http://nlp.stanford.edu/fsnlp/)\n",
    "\n",
    "[D. Jurafsky and J. H. Martin, *Speech and Language Processing* (2016).](https://web.stanford.edu/~jurafsky/slp3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NLP is a really broad field.  Some really imporant NLP tasks that we mostly won't dive into deeply include: \n",
    "* Part of speech tagging (what are the nouns, verbs, adjectives, prepositions).\n",
    "+ Information Extraction\n",
    "+ Sentiment Analysis (determine the attitude of text, e.g., is it positive or negative).\n",
    "+ Semantic Parsing (translate natural language into a formal meaning representation).\n",
    "\n",
    "Generally, the current state-of-the-art for many NLP tasks is to find a good way to represent the text (\"extract features\") and then to use machine learning / statistics tools, such as classification or clustering. \n",
    "\n",
    "Our goal today is to use NLTK + scikit-learn to do some basic NLP tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install datasets and models\n",
    "\n",
    "To use NLTK, you must first download and install the datasets and models. \n",
    "\n",
    "Run `/usr/local/bin/pip3 install nltk` (or whatever setup works for you to install packages)\n",
    "\n",
    "Then execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of NLTK\n",
    "\n",
    "We have downloaded a set of text corpora above. Here is a list of these texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 20 words of that text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby      = text1\n",
    "sense     = text2\n",
    "genesis   = text3\n",
    "inaugural = text4\n",
    "monty     = text6\n",
    "\n",
    "book_names = [ \"Moby\", \"Sense\", \"Genesis\", \"Inaugural\", \"Chat\", \"Monty\", \"WSJ\", \"Personals\", \"Thursday\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Statistics\n",
    "\n",
    "We can check the length of a text. The text of Moby Dick is 260,819 words, whereas Monty Python and the Holy Grail has 16,967 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( moby )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( monty )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check for the frequency of a word. The word \"swallow\" appears 10 times in Monty Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty.count( \"swallow\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to know the context in which \"swallow\" appears in the text\n",
    "\n",
    "\"You shall know a word by the company it keeps.\" – John Firth\n",
    "\n",
    "Use the [`concordance`](http://www.nltk.org/api/nltk.html#nltk.text.Text.concordance) function to print out the words just before and after all occurrences of the word \"swallow\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty.concordance( \"swallow\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we look for Ishmael in Moby Dick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby.concordance( \"Ishmael\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what other words frequently appear in the same context using the  [`similar`](http://www.nltk.org/api/nltk.html#nltk.text.Text.similar) function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty.similar( \"african\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that 'african' and 'unladen' both appeared in the text with the same word just before and just after. To see what the phrase is, we can use the [`common_contexts`](http://www.nltk.org/api/nltk.html#nltk.text.Text.concordance) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty.common_contexts( [\"unladen\", \"african\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both \"an unladen swallow\" and \"an african swallow\" appear in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty.concordance( \"unladen\" )\n",
    "print()\n",
    "monty.concordance( \"african\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion plot\n",
    "\n",
    "`text4` is the Inaugural Address Corpus which includes inaugural addresses going back to 1789. \n",
    "We can use a dispersion plot to see where in a text certain words appear, and hence how the language of the address has changed over time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inaugural.dispersion_plot( [\"citizens\", \"democracy\", \"freedom\", \"duty\", \"America\", \"nation\", \"God\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring texts using statistics\n",
    "\n",
    "We'll explore a text by counting the frequency of different words.\n",
    "\n",
    "The total number of words (\"outcomes\") in Moby Dick is 260,819 and the number of different words is 19,317. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dist = FreqDist( moby )\n",
    "print( frequency_dist )\n",
    "\n",
    "# Find 50 most common words\n",
    "print( '\\n', frequency_dist.most_common(50) )\n",
    "\n",
    "# Not suprisingly, whale occurs quite frequently (906 times!)\n",
    "print( '\\n', frequency_dist['whale'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find all the words in Moby Dick with more than 15 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set( moby )\n",
    "long_words = [ w for w in unique_words if len(w) > 15 ]\n",
    "long_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "Sometimes, it is useful to ignore frequently used words, to concentrate on the meaning of the remaining words. These are referred to as *stopwords*. Examples are \"the\", \"was\", \"is\", etc. \n",
    "\n",
    "NLTK comes with a stopword corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words( 'english' )\n",
    "print( stopwords )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the task, these stopwords are important modifiers, or superfluous content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Frequent Words\n",
    "Find the most frequently used words in Moby Dick that are not stopwords and not punctuation. Hint: [`str.isalpha()`](https://docs.python.org/3/library/stdtypes.html#str.isalpha) could be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist( moby )\n",
    "\n",
    "[ w for w in fd.most_common(150) if w[0].lower() not in stopwords and w[0][0].isalpha() ]\n",
    "#   len([l for l in w[0] if l.isalpha()]) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords in Different Corpora\n",
    "Is there a difference between the frequency in which stopwords appear in the different texts? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_fraction( text ):\n",
    "    stopwords = nltk.corpus.stopwords.words( 'english' )\n",
    "    content = [ w for w in text if w.lower() not in stopwords ]\n",
    "    return len( content ) / len( text )\n",
    "\n",
    "for i,t in enumerate( [text1,text2,text3,text4,text5,text6,text7,text8,text9] ):\n",
    "    print( \"%10s - Content percent: %.1f%%\" % (book_names[i], content_fraction(t)*100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, \"Personals Corpus\" has the most content. Monty Python has more content then Moby Dick..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "A *collocation* is a sequence of words that occur together unusually often, we can retreive these using the [`collocations()`](http://www.nltk.org/api/nltk.html#nltk.text.Text.collocations) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moby.collocation_list() # Bi-grams / Two-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( moby.collocation_list.__doc__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby.collocation_list( window_size = 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis for movie reviews\n",
    "We ask the simple question: Is the attitude of a movie review positive or negative? \n",
    "\n",
    "How can we approach this question?\n",
    "\n",
    "Our data is a corpus consisting of 2000 movie reviews together with the user's sentiment polarity (positive or negative). Our goal is to predict the sentiment polarity from just the review. \n",
    "\n",
    "Of course, this is something that we can do very easily: \n",
    "1. That movie was terrible. -> negative\n",
    "+ That movie was great! -> positive\n",
    "\n",
    "More information about this dataset is available [from this website](https://www.cs.cornell.edu/people/pabo/movie-review-data/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datset contains 1000 positive and 1000 negative movie reviews. \n",
    "\n",
    "The paths to / IDs for the individual reviews are accessible via the fileids() call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.fileids()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the positives or negatives explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( reviews.fileids('pos') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are in fact 1000 positive and 1000 negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviews = len( reviews.fileids() )\n",
    "print( \"Number of reviews:\", num_reviews )\n",
    "print( \"# Positive: %d, # Negative: %d\" % ( len(reviews.fileids('pos')), len(reviews.fileids('neg') ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the review for the third movie. Its a negative review for [The Mod Squad](https://www.rottentomatoes.com/m/mod_squad/), which has a \"rotten\" rating on rotten tomatoes. \n",
    "\n",
    "![Mod Squad at Rotten Tomatoes](mod_squad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name of the file \n",
    "fid = reviews.fileids()[ 2 ]\n",
    "print( fid )\n",
    "\n",
    "print( '\\n', reviews.raw(fid) )\n",
    "\n",
    "print( '\\n', reviews.categories(fid) )\n",
    "\n",
    "print( '\\n', reviews.words(fid) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some sentences that indicate that this is a negative review:\n",
    "\n",
    " * \"it is movies like these that make a jaded movie viewer thankful for the invention of the timex indiglo watch\"\n",
    " * \"sounds like a cool movie , does it not ? after the first fifteen minutes , it quickly becomes apparent that it is not .\" \n",
    " * \"nothing spectacular\"\n",
    " * \"avoid this film at all costs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Custom Algorithm\n",
    "We'll build a sentiment classifier using methods we already know to predicts the label ['neg', 'pos'] from the review text\n",
    "\n",
    "`reviews.categories( file_id )` returns the label ['neg', 'pos'] for that movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ reviews.categories(fid) for fid in reviews.fileids() ] # 'pos' or 'neg'\n",
    "labels = { 'pos':1, 'neg':0 }\n",
    "\n",
    "# Create the labels - 1 for positive, 0 for negative\n",
    "y = [ labels[ x[0] ] for x in categories ]\n",
    "\n",
    "display( y[:10] )\n",
    "display( y[-10:] )\n",
    "\n",
    "display( len(y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we collect all words into a nested array datastructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_words = [ list(reviews.words(fid)) for fid in reviews.fileids() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 10 words of the third review:\n",
    "doc_words[2][1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get all of the words in the reviews and make a FreqDist, pick the most common 2000 words and remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the 2000 most common words in lowercase\n",
    "most_common = nltk.FreqDist( w.lower() for w in reviews.words() ).most_common( 2000 )\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_words = [word_tuple for word_tuple in most_common if word_tuple[0].lower() not in stopwords]\n",
    "\n",
    "# Remove punctuation marks\n",
    "filtered_words = [word_tuple for word_tuple in filtered_words if word_tuple[0].isalpha()]\n",
    "\n",
    "print( \"Number of filtered words:\", len(filtered_words) )\n",
    "filtered_words[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  extract this word list from the frequency tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = [ word_tuple[0] for word_tuple in filtered_words ] # Keep the word, ignore the frequency\n",
    "\n",
    "word_features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that takes a document and returns a list of zeros and ones indicating which of the words in  `word_features` appears in that document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features( document ):\n",
    "    document_words = set( document ) # Throw away duplicates by making a set\n",
    "    features = np.zeros( len( word_features ) ) # Start with a vector of 0's\n",
    "    for i, word in enumerate( word_features ):\n",
    "        features[i] = ( word in document_words )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpFeatures = document_features( monty ) # Using Monty Python as example, but not useful with respect to analyzing a review.\n",
    "print( mpFeatures )\n",
    "# 1, 1, 0 -> includes \"film\" and \"one\", but not \"movie\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just focus on the third document. Which words from `word_features` are in this document? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_doc_2 = document_features( doc_words[2] )\n",
    "print( words_in_doc_2 )\n",
    "\n",
    "inds = np.where( words_in_doc_2 == 1 )[0]\n",
    "print( '\\n', [word_features[i] for i in inds] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build our feature set for all the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros( [num_reviews, len(word_features)] )\n",
    "\n",
    "for i in range( num_reviews ):\n",
    "    X[i,:] = document_features( doc_words[i] )\n",
    "\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X => 2000 by 1821 (# of Reviews by # of Features (words))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a feature vector for each of these reviews that we can use in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have features for each document and labels, **we have a classification problem!** \n",
    "\n",
    "NLTK has a built-in classifier, but we'll use the scikit-learn classifiers we're already familiar with. \n",
    "\n",
    "Let's try k-nearest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 40\n",
    "model = KNeighborsClassifier( n_neighbors=k )\n",
    "scores = cross_val_score( model, X, y, cv=10 )\n",
    "\n",
    "print( scores )\n",
    "\n",
    "# Is ~50% success good?  Why?  60% was good for digits..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC( kernel='rbf', C=20 )\n",
    "scores = cross_val_score( model, X, y, cv=3 )\n",
    "\n",
    "print( scores )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that kNN with these parameters is less accurate than SVM, which is about 80% accurate. Of course, we could now use the cross validation scores to find the optimal parameters, `k` and `C`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's see what our algorithm things about the Mod Squad! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain, XTest, yTrain, yTest = train_test_split( X, y, random_state=1, test_size=0.2 )\n",
    "\n",
    "_ = model.fit( XTrain, yTrain )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_squad = [ X[2] ]\n",
    "mod_squad # <- Display the features (words) in the Mod Squad review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict( mod_squad ) # Returns the label... in this case 0 for negative, 1 for positive review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model says 0 - so a bad review! We have successfully built a classifier that can detect the Mod Squad review as a bad review! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we treat monty python's script as a movie review?\n",
    "#   - Perhaps not a useful example, but kind of a fun though experiment.\n",
    "mpRow = [mpFeatures]\n",
    "print( model.predict( mpRow ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a mis-classified movie. Remember, that the first 1000 movies are negative reviews, so we can just look for the first negative one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict( X[0:10] ) # Look at the first 10 reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review 9, which was misclassified, is for Aberdeen, which has [generally favorable reviews](https://www.rottentomatoes.com/m/aberdeen/). Let's looks at the review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = reviews.fileids()[8]\n",
    "\n",
    "print('\\n', reviews.raw(fid))\n",
    "print('\\n', reviews.categories(fid) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we read this, we can see that this is a negative review, but not a terrible review. Take this sentence for example: \n",
    "\n",
    " * \"if signs & wonders sometimes feels overloaded with ideas , at least it's willing to stretch beyond what we've come to expect from traditional drama\"\n",
    " * \"yet this ever-reliable swedish actor adds depth and significance to the otherwise plodding and forgettable aberdeen , a sentimental and painfully mundane european drama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We could have also used the Classifier from the NLTK library\n",
    "\n",
    "Below is the sentiment analysis from [Ch. 6 of the NLTK book](http://www.nltk.org/book/ch06.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [ (list(reviews.words(fileid)), category)\n",
    "                    for category in reviews.categories() \n",
    "                        for fileid in reviews.fileids(category) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the features from all of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):    \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train_set, test_set and perform classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK gives us 88% accuracy, which isn't bad, but our home-made naive algorithm also achieved a respectable 80%.\n",
    "\n",
    "\n",
    "What improvements could we have made? Obviously, we could have used more data, or – in our home-grown model select words that discriminate between good and bad reviews. We could have used n-grams, e.g., to catch \"not bad\" as a postitive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def document_features2(document):\n",
    "    counter = Counter(document)\n",
    "    document_words = set(document)\n",
    "    features = np.zeros(len(word_features))\n",
    "    for i, word in enumerate(word_features):\n",
    "        features[i] = counter[word]\n",
    "    return features\n",
    "\n",
    "X2 = np.zeros([num_reviews,len(word_features)])\n",
    "for i in range(num_reviews):\n",
    "    X2[i,:] = document_features2(doc_words[i])\n",
    "\n",
    "X2[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='rbf', C=20)\n",
    "scores = cross_val_score(model, X2, y, cv=3)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slightly worse than we did with the \"bag of words\" model.  The number of occurences of words seems to not be particulary useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1, test_size=0.2)\n",
    "decisionTree = tree.DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "tree.plot_tree(decisionTree, filled=True, feature_names=word_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree = tree.DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain, XTest, yTrain, yTest = train_test_split(X2, y, random_state=1, test_size=0.2)\n",
    "decisionTree = tree.DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "pca_model = PCA()\n",
    "X_PCA = pca_model.fit_transform( X )\n",
    "\n",
    "# Rotating the axes... \n",
    "#  - Initially each axes was a word (well, the presence/absence of the word)\n",
    "#  - Now each axis is (sort of) a group of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCASort(col):\n",
    "    indices = list(range(len(word_features)))\n",
    "    return sorted(indices, key= lambda x: abs(pca_model.components_[col][x]))\n",
    "\n",
    "# Determining (according to PCA analysis) which words are the most important\n",
    "pca0 = PCASort(0)\n",
    "for x in pca0[-25:]:\n",
    "    print( \"%s: %.1f%%\" % (word_features[x], pca_model.components_[0][x]*100 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = PCASort(1)\n",
    "for x in pca1[-25:]:\n",
    "    print(word_features[x], pca_model.components_[1][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVM analysis on the rotated data:\n",
    "\n",
    "model = svm.SVC( kernel='rbf', C=20 )\n",
    "\n",
    "scores = cross_val_score(model, X_PCA, y, cv=3)\n",
    "print( scores )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Shape:\", X_PCA.shape )\n",
    "\n",
    "for cols in range( 5, 105, 5 ):\n",
    "    model = svm.SVC( kernel='rbf', C=20 )\n",
    "    # In the cross validation, we throw away a huge percent of the data.  (Eg: Use 5 columns, 10 columns,\n",
    "    #         up to 100 columns (out of 1821))\n",
    "    #    - Remember what PCA has done with each axis...\n",
    "    #      - Accuracy when using < 10% of data is at same level as using entire data set.\n",
    "    scores = cross_val_score( model, X_PCA[:, :cols], y, cv=3 )\n",
    "    print( \"Used\", cols, \"PCA components and got scores\", scores )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that once we turn text into \"points\" (vectors), we don't have to think of it as (direclty) NLP (at least as a first approximation).  We can just use our set of ML regression techniques we have been discussing this semester."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fcd46d531329f49b73a0948faa8aed429eb8dafd35203d9948e8358a307ba0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
